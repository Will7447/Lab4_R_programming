---
title: "How to do a prediction problem using ridgereg() function"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{How to do a prediction problem using ridgereg() function}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Step 1: Set up training and test datasets.
The *chas* variable in dataset **BostonHousing** is a factor variable and has to be converted to a numeric variable at first. 
```{r}
library(mlbench) 
data(BostonHousing)
BostonHousing$chas <- as.numeric(as.character(BostonHousing$chas))
```
The proportion of training dataset to original dataset is 70%.
```{r}
library(caret)
set.seed(42)
index <- createDataPartition(BostonHousing$medv, 
                             p = 0.7, 
                             list = FALSE, 
                             times = 1)
training_set <- BostonHousing[index, ]
test_set <- BostonHousing[-index, ]
```


## Step 2: Fit a linear regression model without forward selection of covariates.
```{r}
linreg_no_forward <- lm(medv~.,data = training_set)
```


## Step 3: Fit a linear regression model with forward selection of covariates.
The package *leaps* is for forward selection of covariates. The best model is chosen based on the adjusted $R^2$.
```{r}
library(leaps)
forward_selection <- regsubsets(medv~., 
                                data = training_set,
                                nvmax = length(training_set),
                                method = "forward",
                                intercept = TRUE)
model_sets <- summary(forward_selection)
best_model_index <- which.max(model_sets$adjr2)
selected_predictors <- names(which(model_sets$which[best_model_index,]))
linreg_with_forward <- lm(medv~., data = training_set[, c(selected_predictors[-1], "medv")])
```

## Step 4: Evaluate the performance of *linreg_with_forward* model on the training dataset.
```{r}
predicted_values <- predict(linreg_with_forward)
actual_values <- training_set$medv

mse <- mean((predicted_values - actual_values)^2)
rmse <- sqrt(mse)

ss_total <- sum((actual_values - mean(actual_values))^2)
ss_residual <- sum((actual_values - predicted_values)^2)
rsquared <- 1 - (ss_residual / ss_total)

metrics <- c("MSE"=mse,"RMSE"=rmse,"R-squared"=rsquared)
print(metrics)
```
## Step 5: Fit a ridge regression model using *ridgereg()* function.
Firstly, create a list called *ridge_model_list* serving as a method in *train* function. In this list, custom *ridgereg()* function from package **lab4package**.
```{r}
library(lab4package)
ridge_model_list <- list(label = "Custom Ridge Regression",
                       library = "lab4package",
                       type = "Regression",
                       parameters = data.frame(parameter = "lambda",
                                               class = "numeric",
                                               label = "Lambda"),
                       grid = function (x, y, len = NULL, search = "grid"){
                         if (search == "grid") {
                           out <- expand.grid(lambda = c(0, 10^seq(-1, -4, length = len - 1)))
                         }
                         else {
                           out <- data.frame(lambda = 10^runif(len, min = -5, 1))
                         }
                         out
                       },
                       fit = function (x, y, wts, param, lev, last, classProbs, ...){
                         lab4package::ridgereg$new(y~x, as.data.frame(cbind(x,y)), lambda = param$lambda)
                       },
                       predict = function (modelFit, newdata, submodels = NULL) {
                         modelFit$predict(newdata)
                       },
                       prob = NULL,
                       sort = function(x){
                         x[order(-x$lambda), ]
                       }
) 
```

Then, fit ridge regression models to the training dataset for different values of λ (set by argument *tuneGrid*). Also, find the best hyperparameter value for λ using 10-fold cross-validation on the training set (set by argument *trControl*).
```{r}
set.seed(42)
ridge_models_set <- train(medv ~ .,
                  data = training_set,
                  method = ridge_model_list,
                  tuneGrid = expand.grid(lambda = c(0,10^seq(-4, 2, by = 0.1))),
                  trControl = trainControl(method = "cv", number = 10))
ridge_models_set
```

## Step 6: Evaluate the performance of all three models on the test dataset.
Firstly, extract the actual values in test dataset named as *actual_test_values* and compute predicted values by three models, named as *predicted_linreg_no_forward*, *predicted_linreg_with_forward*, and *predicted_ridgereg_no_forward*.
```{r}
actual_test_values <- test_set$medv

predicted_linreg_no_forward <- predict(linreg_no_forward, test_set)
predicted_linreg_with_forward <- predict(linreg_with_forward, test_set)

ridgereg_no_forward <- ridgereg$new(medv ~ ., training_set, lambda = 1.995262)
predicted_ridgereg_no_forward <- ridgereg_no_forward$predict(test_set[,-14])

# combine three predicted values as a data frame for metrics calculation.
predict_value_df <- cbind(predicted_linreg_no_forward,predicted_linreg_with_forward,predicted_ridgereg_no_forward) 
```

Then, compute **MSE**, **RMSE**, **R-squared** for each of the models.
```{r}
mse_values <- c()
rmse_values <- c()
rsquared_values <- c()
for (i in 1:length(predict_value_df[1,])){
  mse_values[i] <- mean((predict_value_df[,i]-actual_test_values)^2)
  rmse_values[i] <- sqrt(mse_values[i])
  ss_total <- sum((actual_test_values-mean(actual_test_values))^2)
  ss_residual <- sum((actual_test_values-predict_value_df[,i])^2)
  rsquared_values[i] <- 1-(ss_residual/ss_total)
}
```

Finally, create a data frame for comparison.
```{r}
comparison_df <- cbind(mse_values,rmse_values,rsquared_values)
three_model_names <- c("linreg_no_forward", "linreg_with_forward", "ridgereg_no_forward")
row.names(comparison_df) <- three_model_names
print(comparison_df)
```
Two facts can be concluded by the comparison data frame.  
First, forward selection of covariates can improve performance (with less $MSE$ and $RMSE$ as well as larger $R^2$) in the context of the same type of regression model (linear regression in this case).  
Second, in the absence of forward selection for both, ridge regression model can do a better fit job compared with the linear one.  
As for the linear regression model **with** forward selection and the ridge regression model **without** forward selection, a conclusion cannot be drawn yet because there are two variables involved. Further studies are needed.





